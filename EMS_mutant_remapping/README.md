# Exome capture sequencing data remapping 

Exome capture sequencing data was generated by [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and available through [PRJNA258539](https://www.ncbi.nlm.nih.gov/sra?linkname=bioproject_sra_all&from_uid=258539). This BioProject contains 1,479 SRA experiments, including 39 additional libraries for redundant Kronos mutants. These redundant libraries will be merged, resulting in 1,440 unique mutants for analysis. The datasets are processed with (A) the MAPS pipeline and (B) GATK, separately. 

### 1. Retrieving SRA Accession Numbers
The SRA and corresponding SRR accessions were retrieved for the given SRX accessions as below.
```
from pysradb import SRAweb

db = SRAweb()
with open('SRA.list', 'r') as infile:
    for line in infile:
        mutant, srx = line.strip().split('\t')
        srr = db.srx_to_srr([srx])['run_accession'].iloc[0]
        print(f'{mutant} {srx} {srr}')
```

This information is available in **SRA.list** and looks like this.
```
Mutant ID	SRA accession	SRR accession
Kronos0-1	SRX688079	SRR1559585
Kronos0-1	SRX688078	SRR1559584
Kronos0-2	SRX688080	SRR1559586
Kronos1000	SRX688377	SRR1559883
Kronos1002	SRX688378	SRR1559884
```

### 2. Downloading Sequencing Data

The SRR accessions (${srr}) obtained from the previous is now referred to as ${accession}. Sequencing data is downloaded from the Sequence Read Archive using sra-toolkit v3.1.1. 
```
sratoolkit.3.1.1-centos_linux64/bin/prefetch ${accession}
sratoolkit.3.1.1-centos_linux64/bin/fasterq-dump -O . -e ${Numthreads} ${accession}
```

### 3. Quality Control and Filtering

Reads are filtered using fastp v0.23.4 to remove low-quality reads:
```
fastp --in1 ${accession}_1.fastq --in2 ${accession}_2.fastq --out1 ${accession}.1.filtered.fq --out2 ${accession}.2.filtered.fq --thread 16 -q 20
```

## A. The MAPS pipeline

### 4A. Read Alignment
Reads are aligned to the broken Kronos genome using bwa v0.7.18-r1243-dirty:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

# ${x} is either 1 or 2 for R1 and R2 in the paired-end library
bwa aln -t ${Numthreads} ${reference_dir}/Kronos -f ${accession}.${x}.sai ${accession}.${x}.filtered.fq
bwa sampe -N 10 -n 10 -f ${accession}.sam ${reference_dir}/Kronos ${accession}.1.sai ${accession}.2.sai ${accession}.1.filtered.fq ${accession}.2.filtered.fq
```

### 5A. Sorting and Deduplication
Alignment files are sorted using samtools v1.20 and duplicates are removed with picard v3.0.0:
```
samtools view -@ ${Numthreads} -h -b ${accession}.sam | samtools sort -@ ${Numthreads} > ${accession}.sorted.bam
samtools index ${accession}.sorted.bam

#occasionally, the bam file may contains records that picard does not like to process.
#ALIDATION_STRINGENCY=LENIENT can be added to skip those alignments
picard MarkDuplicates REMOVE_DUPLICATES=true I=${accession}.sorted.bam O=${accession}.sorted.rmdup.bam M=${accession}.rmdup.txt
```

### 6A. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. These are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.sorted.rmdup.bam
 done < merge.list
```

### 7A. Rescuing Multi-mapped Reads
Multi-mapped reads are rescued. We also produce outputs that contain 'unique-only'. For these outputs, these steps are skipped, and multi-mapped reads are excluded.
```
for *.bam; do
    python /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/wheat_tilling_pub/postprocessing/multi_map/multi-map-corrector-V1.6.py -i (< samtools view -@56 ${bam}) -l broken_chr.length.list
```

### 7A. Preparing for MAPS Pipeline
Mutants were sequenced in batches of 8 mutants per preparation and 3 batches per sequencing run, though some variations exist. To keep each batch together for MAPS processing, we create separate folders based on MAPS_groups.list:

```
#create folders and move bam files
for i in {1..60}; do mkdir MAPS-${i}; done

# Move BAM files into their respective groups
cat MAPS_groups.list | while read line; do
    accession=$(echo $line | awk '{print $4}')
    i=$(echo $line | awk '{print $6}')
    mv ${accession}.sorted.rmdup.bam MAPS-${i}/
done
```

### 8A. Running the MAPS Pipeline

For each folder, we will run [the MAPS pipeline](https://github.com/DubcovskyLab/wheat_tilling_pub). First, generate mpileup outputs.
```
python ./wheat_tilling_pub/maps_pipeline/beta-run-mpileup.py \
      -t 30 -r Kronos.collapsed.chromosomes.masked.v1.1.broken.fa \
      -q 20 -Q 20 -o Kronos_mpileup.txt -s $(which samtools) \
      --bamname .sorted.rmdup.bam
```

Then, process the mpileup outputs.
```
mkdir MAPS && cd MAPS
for idx in {2..30}; do # 28 chromosomes + unplaced 
    bed="../temp-region-${idx}.bed"
    mpileup="../temp-mpileup-part-${idx}.txt"

    # Read the first line only and get the first field (chromosome)
    chr=$(awk 'NR==1 {print $1}' "$bed")

    # Check if the chromosome directory exists, if not, create it
    [ ! -d "${chr}" ] && mkdir -p "${chr}"

    # Copy the mpileup file into the appropriate chromosome directory
    # Add error checking for the copy operation
    head -n 1 ../Kronos_mpileup.txt > "${chr}/${chr}_mpileup.txt"
    cat "$mpileup" >> "${chr}/${chr}_mpileup.txt"
done
```

Run the first part of the MAPS pipeline.
```
#the paramter l was (# libraries - 4)
for chr in $(ls -d */ | sed 's/\/$//'); do
    pushd "$chr"  
    python ./wheat_tilling_pub/maps_pipeline/beta-mpileup-parser.py -t 56 -f "${chr}_mpileup.txt"
    python ./wheat_tilling_pub/maps_pipeline/beta-maps1-v2.py -f "parsed_${chr}_mpileup.txt" -t 56 -l ${l} -o "${chr}.mapspart1.txt"
    popd
done
```

Run the second part of the MAPS pipeline.
```
cd .. && mkdir MAPS_output && cd MAPS_output
head -n 1 ../MAPS/1A/1A.mapspart1.txt > all.mapspart1.out
#merge all outcomes
for file in ../MAPS/*/*.mapspart1.txt; do tail -n +2 "$file"; done >> all.mapspart1.out

#diversify two parameters
#each pair is homMinCov,hetMinCov
for pair in "2,3" "3,4" "3,5" "4,6"; do
  k=$(echo $pair | cut -d',' -f1)
  j=$(echo $pair | cut -d',' -f2)
  python ./wheat_tilling_pub/maps_pipeline/maps-part2-v2.py -f all.mapspat1.txt --hetMinPer 15 -l $l --homMinCov $k --hetMinCov $j -o all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.tsv -m m
done
```

### 9A. Final Processing
Once all folders were processed, all outputs were merged together. Make sure that all 1,440 library data is present here. 
```
for tsv in MAPS-1/MAPS_output/*.tsv; do
  cat MAPS-*/MAPS_output/${tsv} > ${tsv}
done
```

Then, convert the SRR accessions to proper Kronos mutant names and the tsv files to vcf formats. 
```
ls *.tsv | while read line; do python reformat_maps2_tsv.py $line ; done
ls *.reformatted.tsv | while read line; bash ./wheat_tilling_pub/postprocessing/residual_heterogeneity/generate_RH.sh $line chr.length.list; done

mkdir no_RH
mv *No_RH.maps* No_RH/ && cd No_RH/
bash wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh

mkdir RH
mv *RH_only* RH/ && cd RH
bash ./wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh
```

Find the paramters that maximize the number of EMS-type substitutions while maintaining EMS mutation rates > 98%. Then, generate the final outputs by collecting mutations detected using those parameters.
```
python summarize_vcf.py
python ../final_vcf.py combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.vcf No_RH.maps.vcf
python ../final_vcf.py combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.vcf RH_only.maps.vcf
```

java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2 combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.vcf > 
combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.snpeff.vcf
java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2 combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.vcf > combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.snpeff.vcf
Finally, let's run varient effect predictions using the v2 annotation set. 



## B. The MAPS pipeline

### 4B. Read Alignment
Reads are aligned to the broken Kronos genome using bwa v0.7.18-r1243-dirty:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

for read1 in *.1.filtered.fq; do
  accession="${read1%.1.filtered.fq}"
  read2="${accession}.2.filtered.fq"

  # Add read group information during alignment
  RG="@RG\tID:${prefix}\tLB:${prefix}\tPL:ILLUMINA\tPU:unit1\tSM:${accession}"
  bwa mem -t 56 -R "$RG" /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos "${read1}" "${read2}" > "${accession}.gatk.sam"
```

### 5B. Sorting and Deduplication
Alignment files are sorted using samtools v1.20 and duplicates are removed with picard v3.0.0:
```
  samtools view -@56 -h "${accession}.gatk.sam" | samtools sort -@56 -o "${accession}.gatk.sorted.bam"
  samtools index -@56 "${accession}.gatk.sorted.bam"
  picard MarkDuplicates REMOVE_DUPLICATES=true I="${accession}.gatk.sorted.bam" O="${accession}.gatk.sorted.rmdup.bam" M="${accession}.rmdup.txt"
```

### 6B. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. these are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.gatk.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.gatk.sorted.rmdup.bam
 done < merge.list
```

### 7B. Running HaplotypeCaller
SNPs are called by HaplotypeCaller from GATK v4.5.0, and positions are adjusted for the intact chromosomes.
```
for bam in *.gatk.sorted.rmdup.bam; do
  accession="${bam%.gatk.sorted.rmdup.bam}"
  gatk HaplotypeCaller -R /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos.collapsed.chromosomes.masked.v1.1.broken.fa -I ${accession}.gatk.sorted.rmdup.bam -O ${accession}.vcf.gz
  gunzip ${accession}.vcf.gz
  python reformat_gatk.py ${accession}.vcf
done
```

### 8B. Variant Effect Prediction
Variant effect prediction is run on the annotation set version 2. 
```
for vcf in Kronos*.vcf; do
    accession="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2  ${vcf}> ${accession}.gatk.exome.snpeff.vcf
done
```


## Outputs

```
${mutant}.gatk.exome.snpeff.vcf: Exome-capture data processed with the GATK pipeline
```
