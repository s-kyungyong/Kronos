# Capture sequencing data remapping 

## Data availability 
Processed exome and promoter-capture data are available through Zenodo 
â€¢ `Exome capture (MAPS) [âœ¨Finalâœ¨]`:  
â€¢ `Exome capture (GATK) [âœ¨Finalâœ¨]`:  
â€¢ `Promoter capture (MAPS) [âœ¨Finalâœ¨]`:  
â€¢ `Promoter capture (GATK) [âœ¨Finalâœ¨]`:  

## Software version
```
sra-toolkit v3.1.1
fastp v0.23.4
trimmomatic v0.39
bwa v0.7.18-r1243-dirty
samtools v1.20
picard v3.0.0
MAPS
GATK v4.5.0
snpeff v5.2a
```
---

## Exome-capture Data Remapping

Exome capture sequencing datasets were  generated by [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and were processed with (A) the MAPS pipeline and (B) GATK, separately. 

### 1. Downloading Sequencing Data

ðŸ“¥ Inputs  
â€¢ `exome_MAPS_groups.list`: SRA identifiers for exome-capture data  

ðŸ“¥ Outputs  
â€¢ `*.fastq`: paired-end fastq files  

The SRR accessions in **exome_MAPS_groups.list** is now referred to as ${accession}. 
```
sratoolkit.3.1.1-centos_linux64/bin/prefetch ${accession}
sratoolkit.3.1.1-centos_linux64/bin/fasterq-dump -O . -e ${Numthreads} ${accession}
```
---

### 2. Quality Control and Filtering

ðŸ“¥ Inputs  
â€¢ `*.fastq`: paired-end fastq files  

ðŸ“¥ Outputs  
â€¢ `*.filtered.fastq`: filtered and trimmed paired-end fastq files  

```
fastp --in1 ${accession}_1.fastq --in2 ${accession}_2.fastq --out1 ${accession}.1.filtered.fq --out2 ${accession}.2.filtered.fq --thread 16 -q 20
```

---
### 3. (MAPS) Read Alignment

ðŸ“¥ Inputs   
â€¢ `*.filtered.fastq`: filtered and trimmed paired-end fastq files   


ðŸ“¥ Outputs   
â€¢ `Kronos.collapsed.chromosomes.masked.v1.1.broken.fa`: Kronos reference genome v1.1 (more about this genome is [here](https://github.com/krasileva-group/Kronos_EDR)).  
â€¢ `*.sam`: alignment files  

```
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa
```

```
#for each accession
for i in 1 2; do
    bwa aln -t ${Numthreads} ${reference_dir}/Kronos -f ${accession}.${x}.sai ${accession}.${x}.filtered.fq
done
bwa sampe -N 10 -n 10 -f ${accession}.sam ${reference_dir}/Kronos ${accession}.1.sai ${accession}.2.sai ${accession}.1.filtered.fq ${accession}.2.filtered.fq
```
---
### 4. (MAPS) Sorting and Deduplication

ðŸ“¥ Inputs   
â€¢ `*.sam`: alignment files  

ðŸ“¥ Outputs    
â€¢ `*.sorted.rmdup.sam`: sorted alignment files without duplicate reads  

```
samtools view -@ ${Numthreads} -h -b ${accession}.sam | samtools sort -@ ${Numthreads} > ${accession}.sorted.bam
samtools index ${accession}.sorted.bam
```

```
#occasionally, the bam file may contains records that picard does not like to process.
#ALIDATION_STRINGENCY=LENIENT can be added to skip those alignments
picard MarkDuplicates REMOVE_DUPLICATES=true I=${accession}.sorted.bam O=${accession}.sorted.rmdup.bam M=${accession}.rmdup.txt
```
---
### 5. (MAPS) Merging Redundant Libraries

ðŸ“¥ Inputs   
â€¢ `*.sorted.rmdup.sam`: sorted alignment files without duplicate reads  

ðŸ“¥ Outputs    
â€¢ `*.sorted.rmdup.sam`: merged sorted alignment files without duplicate read  
â€¢ `exome_merge.list`: a list of alignments to be merged together 

```
#Some mutants have multiple sequencing datasets. These are merged into a single BAM file prior to the MAPS piepline
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.sorted.rmdup.bam
 done < exome_merge.list
```
---
### 6. (MAPS) Preparing for MAPS Pipeline

ðŸ“¥ Inputs   
â€¢ `exome_MAPS_groups.list`: A list of alignments to be processed together as a batch 

ðŸ“¥ Outputs    
â€¢ `MAPS-{x}`: folders containing each batch of alignments to be processed together

```
#create folders and move bam files
for i in {1..60}; do mkdir MAPS-${i}; done

# Move BAM files into their respective groups
cat exome_MAPS_groups.list | while read line; do
    accession=$(echo $line | awk '{print $4}')
    i=$(echo $line | awk '{print $6}')
    mv ${accession}.sorted.rmdup.bam MAPS-${i}/
done
```

---
### 7. (MAPS) Running the MAPS Pipeline

ðŸ“¥ Inputs   
â€¢ `*.sorted.rmdup.bam`: sorted alignment files without duplicate reads  

ðŸ“¥ Outputs    
â€¢ `Kronos_mpileup.txt`: mpile up output for the MAPS pipeline part 1 

âš™ï¸ **Mpileup**  
Note: the MAPS pipeline is available [here](https://github.com/DubcovskyLab/wheat_tilling_pub). 
```
#for each folder
python ./wheat_tilling_pub/maps_pipeline/beta-run-mpileup.py \
      -t 30 -r Kronos.collapsed.chromosomes.masked.v1.1.broken.fa \
      -q 20 -Q 20 -o Kronos_mpileup.txt -s $(which samtools) \
      --bamname .sorted.rmdup.bam
```

âš™ï¸ **MAPS Part 1**  
Note: set l as **(# libraries - 4)**.
```
mkdir MAPS && cd MAPS
for idx in {2..30}; do # 28 chromosomes + unplaced 
    bed="../temp-region-${idx}.bed"
    mpileup="../temp-mpileup-part-${idx}.txt"

    # Read the first line only and get the first field (chromosome)
    chr=$(awk 'NR==1 {print $1}' "$bed")

    # Check if the chromosome directory exists, if not, create it
    [ ! -d "${chr}" ] && mkdir -p "${chr}"

    # Copy the mpileup file into the appropriate chromosome directory
    # Add error checking for the copy operation
    head -n 1 ../Kronos_mpileup.txt > "${chr}/${chr}_mpileup.txt"
    cat "$mpileup" >> "${chr}/${chr}_mpileup.txt"
done
```

```
for chr in $(ls -d */ | sed 's/\/$//'); do
    pushd "$chr"  
    python ./wheat_tilling_pub/maps_pipeline/beta-mpileup-parser.py -t 56 -f "${chr}_mpileup.txt"
    python ./wheat_tilling_pub/maps_pipeline/beta-maps1-v2.py -f "parsed_${chr}_mpileup.txt" -t 56 -l ${l} -o "${chr}.mapspart1.txt"
    popd
done
```

âš™ï¸ **MAPS Part 2**  
```
cd .. && mkdir MAPS_output && cd MAPS_output
head -n 1 ../MAPS/1A/1A.mapspart1.txt > all.mapspart1.out
#merge all outcomes
for file in ../MAPS/*/*.mapspart1.txt; do tail -n +2 "$file"; done >> all.mapspart1.out

#diversify two parameters
#each pair is homMinCov,hetMinCov
for pair in "2,3" "3,4" "3,5" "4,6"; do
  k=$(echo $pair | cut -d',' -f1)
  j=$(echo $pair | cut -d',' -f2)
  python ./wheat_tilling_pub/maps_pipeline/maps-part2-v2.py -f all.mapspat1.txt --hetMinPer 15 -l $l --homMinCov $k --hetMinCov $j -o all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.tsv -m m
done
```
---
### 8. (MAPS) VCF Conversion

ðŸ“¥ Inputs   
â€¢ `all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.tsv`: called mutations in tsv format from four pairs of parameters

ðŸ“¥ Outputs    
â€¢ `all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.reformatted.tsv`: reformatted mutations


```
#concatnate all outputs
for tsv in MAPS-1/MAPS_output/*.tsv; do
  cat MAPS-*/MAPS_output/${tsv} > ${tsv}
done

#adjust coordinates for mutation: broken genome -> full genome
#change identifier: SRRXXX -> Kronos3412
ls *.tsv | while read line; do python reformat_maps2_tsv.py $line exome_MAPS_groups.list; done
```
---
### 9. (MAPS) Separate Regions with Residual Hetrogenity

ðŸ“¥ Inputs   
â€¢ `all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.reformatted.tsv`: reformatted mutations

ðŸ“¥ Outputs    
â€¢ `all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.vcf`: mutations not from residual hetrogenity regions
â€¢ `all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.vcf`: mutations from residual hetrogenity regions


```
Separate regions with residual hetrogenity.
ls *.reformatted.tsv | while read line; bash ./wheat_tilling_pub/postprocessing/residual_heterogeneity/generate_RH.sh $line chr.length.list; done

mkdir no_RH
mv *No_RH.maps* No_RH/ && cd No_RH/
bash wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh

mkdir RH
mv *RH_only* RH/ && cd RH
bash ./wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh
```

---
### 9. (MAPS) Summarizing Mutations Per Prameter

ðŸ“¥ Inputs   
â€¢ `all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.vcf`: mutations **not from** residual hetrogenity regions

ðŸ“¥ Outputs    
â€¢ `Mutations.summary`: mutation numbers per sample per parameter  

```
The MAPS outputs were generated with four pairs of HomMinCov and HetMinCov: HetMinCov3HomMinCov2, HetMinCov4HomMinCov3, HetMinCov5HomMinCov3 and HetMinCov6HomMinCov4 from the least to most stringency.
If any of the thresholds yielded â‰¥ 98% EMS rates, the following criteria are applied.

High confidence: the least stringent threshold yielding â‰¥ 98% EMS rates
Medium confidence: the least stringent threshold yielding â‰¥ 97% EMS rates among the remainning three, N/A otherwise
Low confidence: the least stringent threshold yielding â‰¥ 95% EMS rates among the remainning ones, N/A otherwise


If none of the thresholds yielded â‰¥ 98% EMS rates, pre-defined confidence levels will be used as [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114).
```
High confidence: HetMinCov5HomMinCov3
Medium confidence: HetMinCov4HomMinCov3
Low confidence: HetMinCov3HomMinCov2
```

Summarize the statistics and select the parameters. This will create Mutations. summary
```
#run this within the No_RH folder.
python summarize_vcf.py
```

### 10A. Rescuing Multi-mapped Reads

Almost everything is ready. We finally need to rescue multi-mapped reads. This step goes back to Step 5A and requires *.sorted.rmdup.bam files. Multi-mapped reads are rescored, so that any meaningful mutations can be picked up by the MAPS pipeline. 
```
for bam in *.sorted.rmdup.bam; do
    prefix="${bam%.bam}"

    # Convert BAM to SAM and extract header
    samtools view -@56 -h "${bam}" > "${prefix}.sam"
    samtools view -H "${prefix}.sam" > header.sam

    # Run multi-map corrector
    python ./wheat_tilling_pub/postprocessing/multi_map/multi-map-corrector-V1.6.py -i "${prefix}.sam" -l broken_chr.length.list > "${prefix}.rescued.sam"

    # Merge header and body, then convert to BAM
    cat header.sam "${prefix}.rescued.sam" | samtools view -@ 56 -b -o "${prefix}.rescued.bam"

    # Clean up temporary files
    rm "${prefix}.sam" "${prefix}.rescued.sam" header.sam
done
```

Once these files are generated, the entire MAPS pipeline from 6A to 8A is rerun for them. 

### 11A. Finalization

All outputs are concatnated into four final outputs. With the folders structured as following:
```
ls TSVs #uniquely mapped reads
No_RH  RH_only

ls TSVs-Multi #multi-mapped reads
No_RH  RH_only
```


Run the script.
```
python finalize_vcf.py
```

We will create four final outputs! 
```
# Excluding genomic regtions with residual hetrogenity. Indels only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.No_RH.maps.indels.snpeff.vcf
# Excluding genomic regtions with residual hetrogenity. Substitutions only ------- This is our primary output 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.No_RH.maps.substitutions.snpeff.vcf
# Only genomic regtions with residual hetrogenity. Indels only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.RH_only.maps.indels.snpeff.vcf
# Only genomic regtions with residual hetrogenity. Substitutions only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.RH_only.maps.substitutions.snpeff.vcf
```

Within these vcf files, the following fields will be included. Our primary mutations are labeled with Substitution/False/High/Any threshold/Unique. 
```
Type={Substiution/Indel}: This indicates whether called mutations are substitutions or indels. 
RH={True/False}: This indicates whether called mutations come from regions associated with residual hetrogenity.  
Confidence={High/Medium/Low}: This indicates the confidence level of called mutations.
Threshold=HetMinCov{x}HomMinCov{y}: This indicates parameters used to call the mutations. 
Mapping={Unique/Multi}: This indicates whether called mutations come from uniquely mapped or multi-mapped reads. 
```

### 12A. snpEff

Finally, snpEff is run to add variant effect prediction outputs. This is based on the annotation v21. 
```
for vcf in *.vcf:
    prefix="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv21 ${vcf} > ${prefix}.snpeff.vcf
done
```


## B. The GATK pipeline

To call variants with GATK, we begin with the trimmed filtered sequencing outputs from Step 2. 

### 3B. Read Alignment
Reads are aligned to the broken Kronos genome using bwa mem:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

for read1 in *.1.filtered.fq; do
  accession="${read1%.1.filtered.fq}"
  read2="${accession}.2.filtered.fq"

  # Add read group information during alignment
  RG="@RG\tID:${accession}\tLB:${accession}\tPL:ILLUMINA\tPU:unit1\tSM:${accession}"
  bwa mem -t 56 -R "$RG" /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos "${read1}" "${read2}" > "${accession}.gatk.sam"
```

### 4B. Sorting and Deduplication
Alignment files are sorted using samtools and duplicates are removed with picard:
```
  samtools view -@56 -h "${accession}.gatk.sam" | samtools sort -@56 -o "${accession}.gatk.sorted.bam"
  samtools index -@56 "${accession}.gatk.sorted.bam"
  picard MarkDuplicates REMOVE_DUPLICATES=true I="${accession}.gatk.sorted.bam" O="${accession}.gatk.sorted.rmdup.bam" M="${accession}.rmdup.txt"
```

### 5B. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. these are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.gatk.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.gatk.sorted.rmdup.bam
 done < exome_merge.list
```

### 6B. Running HaplotypeCaller
SNPs are called by HaplotypeCaller from GATK, and positions are adjusted for the intact chromosomes.
```
for bam in *.gatk.sorted.rmdup.bam; do
  accession="${bam%.gatk.sorted.rmdup.bam}"
  gatk HaplotypeCaller -R /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos.collapsed.chromosomes.masked.v1.1.broken.fa -I ${accession}.gatk.sorted.rmdup.bam -O ${accession}.vcf.gz
  gunzip ${accession}.vcf.gz
  python reformat_gatk.py ${accession}.vcf
done
```

### 7B. Variant Effect Prediction
Variant effect prediction is run on the annotation version 2.1. 
```
for vcf in Kronos*.vcf; do
    accession="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2  ${vcf}> ${accession}.gatk.exome.snpeff.vcf
done
```

---


# Promoter capture sequencing data remapping 

The analysis of promoter-capture sequencing data was done nearly identically with the exome capture data analysis. Thus, only the workflow different from the previous one is recorded below. 


### 1. Downloading Sequencing Data
Most of the sequencing data were directly obtained from [Zhang et al. 2023](https://www.pnas.org/doi/10.1073/pnas.2306494120). Any missing sequencing data were downloaded from PRJNA1218005.

### 2. Quality Control and Filtering
The datasets in PRJNA1218005 should already be trimmed. A small number of datasets directly provided by the author was not. These were trimmed with trimmomatic. 

```
for fq1 in *R1*fastq.gz; do
        fq2=$(echo $fq1 | sed 's/R1/R2/g')
        out1=$(echo $fq1 | sed 's/.fastq.gz/_trimmed.fq.gz/g')
        out2=$(echo $fq2 | sed 's/.fastq.gz/_trimmed.fq.gz/g')

        if [[ ! -f "${out1}" ]]; then
                trimmomatic PE $fq1 $fq2 $out1 ${fq1}_trimmed_unpaired.fq.gz $out2 ${fq2}_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 SLIDINGWINDOW:4:20
        fi
done
```

### 5A. Merging Redundant Libraries

Although some Kronos mutants were sequenced multiple times, we did not concatenate redundant libraries into single files, unlike the approach taken for the exome capture data. Each batch and the mutant libraries within it were clearly defined by Junli et al., and we wanted to preserve this original structure. As a result, datasets generated from the same mutant were only merged at the final processing stage. Overall, 1,518 mutants have a single associated sequencing dataset, and 38 mutants have two. 

For GATK outputs, finish running Step 6B. Then, the vcf files can be merged by looking for unions of mutations:
```
awk '{print $2}' promoter_MAPS_group.list | grep 'Kronos' | sort | uniq -c | awk '$1 != 1 {print $2}' > redundant.list
while read -r accession; do
    libs=$(ls */${accession}.reformatted.vcf)
    for lib in ${libs}; do
            bgzip -c $lib > ${lib}.gz
            tabix --csi -p vcf ${lib}.gz
    done
    
    bcftools isec -o merged/${accession}.merged.reformatted.vcf.gz -w 1 -Oz -n =2 ${libs}
    bcftools norm -m -both merged/${accession}.merged.reformatted.vcf.gz -f Kronos.collapsed.chromosomes.masked.v1.1.fa -Ov -o merged/${accession}.merged.norm.reformatted.vcf
done < redundant.list
```

### 6A. Preparing for MAPS Pipeline

The BioProject PRJNA1218005 contains 33 BioSamples. Each BioSample contains a batch of Kronos mutants to be processed together. 

### 7A. Running the MAPS Pipeline

In first part of the MAPS pipeline, l was set as **int(math.ceil(lib * 0.8))**.
