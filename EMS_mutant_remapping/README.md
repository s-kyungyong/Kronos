# Capture sequencing data remapping 


## Data availability 
Exome and promoter capture data come from [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and [Zhang et al., 2023](https://www.pnas.org/doi/10.1073/pnas.2306494120).
```
Exome capture: PRJNA258539
Promoter capture: PRJNA1218005 
```

Variants detected by remapping to the Kronos reference genome are available in Zenodo. The annotation v2.1 was used to retrieve mutation effects. 
```
```


## Methods


---

## Exome-capture

Exome capture sequencing data was generated by [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and available through [PRJNA258539](https://www.ncbi.nlm.nih.gov/sra?linkname=bioproject_sra_all&from_uid=258539). This BioProject contains 1,479 SRA experiments, including 39 additional libraries for redundant Kronos mutants. These redundant libraries will be merged, resulting in 1,440 unique mutants for analysis. The datasets are processed with (A) the MAPS pipeline and (B) GATK, separately. 

### 1. Retrieving SRA Accession Numbers
The SRA and corresponding SRR accessions were retrieved for the given SRX accessions as below.
```
from pysradb import SRAweb

db = SRAweb()
with open('SRA.list', 'r') as infile:
    for line in infile:
        mutant, srx = line.strip().split('\t')
        srr = db.srx_to_srr([srx])['run_accession'].iloc[0]
        print(f'{mutant} {srx} {srr}')
```

This information is available in **SRA.list** and looks like this.
```
Mutant ID	SRA accession	SRR accession
Kronos0-1	SRX688079	SRR1559585
Kronos0-1	SRX688078	SRR1559584
Kronos0-2	SRX688080	SRR1559586
Kronos1000	SRX688377	SRR1559883
Kronos1002	SRX688378	SRR1559884
```

### 2. Downloading Sequencing Data

The SRR accessions (${srr}) obtained from the previous is now referred to as ${accession}. Sequencing data is downloaded from the Sequence Read Archive using sra-toolkit v3.1.1. 
```
sratoolkit.3.1.1-centos_linux64/bin/prefetch ${accession}
sratoolkit.3.1.1-centos_linux64/bin/fasterq-dump -O . -e ${Numthreads} ${accession}
```

### 3. Quality Control and Filtering

Reads are filtered using fastp v0.23.4 to remove low-quality reads:
```
fastp --in1 ${accession}_1.fastq --in2 ${accession}_2.fastq --out1 ${accession}.1.filtered.fq --out2 ${accession}.2.filtered.fq --thread 16 -q 20
```

## A. The MAPS pipeline

### 4A. Read Alignment
Reads are aligned to the broken Kronos genome using bwa v0.7.18-r1243-dirty:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

# ${x} is either 1 or 2 for R1 and R2 in the paired-end library
bwa aln -t ${Numthreads} ${reference_dir}/Kronos -f ${accession}.${x}.sai ${accession}.${x}.filtered.fq
bwa sampe -N 10 -n 10 -f ${accession}.sam ${reference_dir}/Kronos ${accession}.1.sai ${accession}.2.sai ${accession}.1.filtered.fq ${accession}.2.filtered.fq
```

### 5A. Sorting and Deduplication
Alignment files are sorted using samtools v1.20 and duplicates are removed with picard v3.0.0:
```
samtools view -@ ${Numthreads} -h -b ${accession}.sam | samtools sort -@ ${Numthreads} > ${accession}.sorted.bam
samtools index ${accession}.sorted.bam

#occasionally, the bam file may contains records that picard does not like to process.
#ALIDATION_STRINGENCY=LENIENT can be added to skip those alignments
picard MarkDuplicates REMOVE_DUPLICATES=true I=${accession}.sorted.bam O=${accession}.sorted.rmdup.bam M=${accession}.rmdup.txt
```

### 6A. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. These are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.sorted.rmdup.bam
 done < merge.list
```

### 7A. Preparing for MAPS Pipeline
Mutants were sequenced in batches of 8 mutants per preparation and 3 batches per sequencing run, though some variations exist. To keep each batch together for MAPS processing, we create separate folders based on MAPS_groups.list:

```
#create folders and move bam files
for i in {1..60}; do mkdir MAPS-${i}; done

# Move BAM files into their respective groups
cat MAPS_groups.list | while read line; do
    accession=$(echo $line | awk '{print $4}')
    i=$(echo $line | awk '{print $6}')
    mv ${accession}.sorted.rmdup.bam MAPS-${i}/
done
```

### 8A. Running the MAPS Pipeline
For each folder, we will run [the MAPS pipeline](https://github.com/DubcovskyLab/wheat_tilling_pub). First, generate mpileup outputs.
```
python ./wheat_tilling_pub/maps_pipeline/beta-run-mpileup.py \
      -t 30 -r Kronos.collapsed.chromosomes.masked.v1.1.broken.fa \
      -q 20 -Q 20 -o Kronos_mpileup.txt -s $(which samtools) \
      --bamname .sorted.rmdup.bam
```

Then, process the mpileup outputs.
```
mkdir MAPS && cd MAPS
for idx in {2..30}; do # 28 chromosomes + unplaced 
    bed="../temp-region-${idx}.bed"
    mpileup="../temp-mpileup-part-${idx}.txt"

    # Read the first line only and get the first field (chromosome)
    chr=$(awk 'NR==1 {print $1}' "$bed")

    # Check if the chromosome directory exists, if not, create it
    [ ! -d "${chr}" ] && mkdir -p "${chr}"

    # Copy the mpileup file into the appropriate chromosome directory
    # Add error checking for the copy operation
    head -n 1 ../Kronos_mpileup.txt > "${chr}/${chr}_mpileup.txt"
    cat "$mpileup" >> "${chr}/${chr}_mpileup.txt"
done
```

Run the first part of the MAPS pipeline.
```
#the paramter l was (# libraries - 4)
for chr in $(ls -d */ | sed 's/\/$//'); do
    pushd "$chr"  
    python ./wheat_tilling_pub/maps_pipeline/beta-mpileup-parser.py -t 56 -f "${chr}_mpileup.txt"
    python ./wheat_tilling_pub/maps_pipeline/beta-maps1-v2.py -f "parsed_${chr}_mpileup.txt" -t 56 -l ${l} -o "${chr}.mapspart1.txt"
    popd
done
```

Run the second part of the MAPS pipeline.
```
cd .. && mkdir MAPS_output && cd MAPS_output
head -n 1 ../MAPS/1A/1A.mapspart1.txt > all.mapspart1.out
#merge all outcomes
for file in ../MAPS/*/*.mapspart1.txt; do tail -n +2 "$file"; done >> all.mapspart1.out

#diversify two parameters
#each pair is homMinCov,hetMinCov
for pair in "2,3" "3,4" "3,5" "4,6"; do
  k=$(echo $pair | cut -d',' -f1)
  j=$(echo $pair | cut -d',' -f2)
  python ./wheat_tilling_pub/maps_pipeline/maps-part2-v2.py -f all.mapspat1.txt --hetMinPer 15 -l $l --homMinCov $k --hetMinCov $j -o all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.tsv -m m
done
```

### 9A. Vcf Conversion and Residual Hetrogenity Filtering
Once all folders were processed, all outputs were merged together. Make sure that all 1,440 library data is present here. 
```
for tsv in MAPS-1/MAPS_output/*.tsv; do
  cat MAPS-*/MAPS_output/${tsv} > ${tsv}
done
```

Then, convert the SRR accessions to proper Kronos mutant names and the tsv files to vcf formats. 
```
ls *.tsv | while read line; do python reformat_maps2_tsv.py $line ; done
ls *.reformatted.tsv | while read line; bash ./wheat_tilling_pub/postprocessing/residual_heterogeneity/generate_RH.sh $line chr.length.list; done
```

Separate regions with residual hetrogenity.
```
#the outputs from this step will be the main ones. 
mkdir no_RH
mv *No_RH.maps* No_RH/ && cd No_RH/
bash wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh

mkdir RH
mv *RH_only* RH/ && cd RH
bash ./wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh
```

### 10A. Parameter Search
The MAPS outputs were generated with four pairs of HomMinCov and HetMinCov, HetMinCov3HomMinCov2, HetMinCov4HomMinCov3, HetMinCov5HomMinCov3 and HetMinCov6HomMinCov4 from the least to most stringency. Each parameter will be analyzed to select high, medium and low confidence thresholds. There are two major criteria. 

If any of the thresholds yielded ≥ 98% EMS rates, the following criteria are applied. The selection of the least stringent parameter with ≥ 98% EMS rates was initially introduced by [Zhang et al.](https://www.pnas.org/doi/10.1073/pnas.2306494120). Most mutations are picked up by the high confidence threshold, and small numbers are added by medium and low confidence parameters. 
```
High confidence: the least stringent threshold yielding ≥ 98% EMS rates
Medium confidence: the least stringent threshold yielding ≥ 97% EMS rates among the remainning three, N/A otherwise
Low confidence: the least stringent threshold yielding ≥ 95% EMS rates among the remainning three, N/A otherwise
```

If none of the thresholds yielded ≥ 98% EMS rates, pre-defined confidence levels will be used as [Krasileva et al.,](https://www.pnas.org/doi/10.1073/pnas.1619268114).
```
High confidence: HetMinCov5HomMinCov3
Medium confidence: HetMinCov4HomMinCov3
Low confidence: HetMinCov3HomMinCov2
```

Summarize the statistics and select the parameters. This will create Mutations. summary
```
#run this within the No_RH folder.
python summarize_vcf.py
```

### 11A. Rescuing Multi-mapped Reads

Almost everything is ready. We finally need to rescue multi-mapped reads. This step goes back to Step 6A and requires *.sorted.rmdup.bam files. Multi-mapped reads are rescored, so that any meaningful mutations can be picked up by the MAPS pipeline. 
```
for bam in *.sorted.rmdup.bam; do
    prefix="${bam%.bam}"

    # Convert BAM to SAM and extract header
    samtools view -@56 -h "${bam}" > "${prefix}.sam"
    samtools view -H "${prefix}.sam" > header.sam

    # Run multi-map corrector
    python ./wheat_tilling_pub/postprocessing/multi_map/multi-map-corrector-V1.6.py -i "${prefix}.sam" -l broken_chr.length.list > "${prefix}.rescued.sam"

    # Merge header and body, then convert to BAM
    cat header.sam "${prefix}.rescued.sam" | samtools view -@ 56 -b -o "${prefix}.rescued.bam"

    # Clean up temporary files
    rm "${prefix}.sam" "${prefix}.rescued.sam" header.sam
done
```

Once these files are generated, the entire MAPS pipeline from 8A to 9A is rerun for them. 

### 12A. Finalization

All outputs are concatnated into four final outputs.
```
# Excluding genomic regtions with residual hetrogenity. Indels only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.No_RH.maps.indels.snpeff.vcf
# Excluding genomic regtions with residual hetrogenity. Substitutions only ------- This is our primary output 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.No_RH.maps.substitutions.snpeff.vcf
# Only genomic regtions with residual hetrogenity. Indels only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.RH_only.maps.indels.snpeff.vcf
# Only genomic regtions with residual hetrogenity. Substitutions only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.RH_only.maps.substitutions.snpeff.vcf
```

Within these vcf files, the following fields will be included. Our primary mutations are labeled with Substitution/False/High/Any threshold/Unique. 
```
Type={Substiution/Indel}: This indicates whether called mutations are substitutions or indels. 
RH={True/False}: This indicates whether called mutations come from regions associated with residual hetrogenity.  
Confidence={High/Medium/Low}: This indicates the confidence level of called mutations.
Threshold=HetMinCov{x}HomMinCov{y}: This indicates parameters used to call the mutations. 
Mapping={Unique/Multi}: This indicates whether called mutations come from uniquely mapped or multi-mapped reads. 
```

With the folders structured as following:
```
ls TSVs #uniquely mapped reads
No_RH  RH_only

ls TSVs-Multi #multi-mapped reads
No_RH  RH_only
```
Run the script
```
python finalize_vcf.py
```

Finally, snpEff is run to add variant effect prediction outputs. This is based on the annotation v2. 
```
for vcf in *.vcf:
    prefix="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2 ${vcf} > ${prefix}.snpeff.vcf
done
```



## B. The GATK pipeline

### 4B. Read Alignment
Reads are aligned to the broken Kronos genome using bwa mem v0.7.18-r1243-dirty:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

for read1 in *.1.filtered.fq; do
  accession="${read1%.1.filtered.fq}"
  read2="${accession}.2.filtered.fq"

  # Add read group information during alignment
  RG="@RG\tID:${prefix}\tLB:${prefix}\tPL:ILLUMINA\tPU:unit1\tSM:${accession}"
  bwa mem -t 56 -R "$RG" /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos "${read1}" "${read2}" > "${accession}.gatk.sam"
```

### 5B. Sorting and Deduplication
Alignment files are sorted using samtools v1.20 and duplicates are removed with picard v3.0.0:
```
  samtools view -@56 -h "${accession}.gatk.sam" | samtools sort -@56 -o "${accession}.gatk.sorted.bam"
  samtools index -@56 "${accession}.gatk.sorted.bam"
  picard MarkDuplicates REMOVE_DUPLICATES=true I="${accession}.gatk.sorted.bam" O="${accession}.gatk.sorted.rmdup.bam" M="${accession}.rmdup.txt"
```

### 6B. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. these are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.gatk.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.gatk.sorted.rmdup.bam
 done < merge.list
```

### 7B. Running HaplotypeCaller
SNPs are called by HaplotypeCaller from GATK v4.5.0, and positions are adjusted for the intact chromosomes.
```
for bam in *.gatk.sorted.rmdup.bam; do
  accession="${bam%.gatk.sorted.rmdup.bam}"
  gatk HaplotypeCaller -R /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos.collapsed.chromosomes.masked.v1.1.broken.fa -I ${accession}.gatk.sorted.rmdup.bam -O ${accession}.vcf.gz
  gunzip ${accession}.vcf.gz
  python reformat_gatk.py ${accession}.vcf
done
```

### 8B. Variant Effect Prediction
Variant effect prediction is run on the annotation set version 2. 
```
for vcf in Kronos*.vcf; do
    accession="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2  ${vcf}> ${accession}.gatk.exome.snpeff.vcf
done
```




# Promoter capture sequencing data remapping 

The analysis of promoter-capture sequencing data was done nearly identically with the exome capture data analysis. Thus, only the workflow different from the previous one is recorded. 


### 2. Downloading Sequencing Data
The sequencing data were directly obtained from [Zhang et al.](https://www.pnas.org/doi/10.1073/pnas.2306494120) who generated the data. The exceptions were accessions included in the following repositories, which were downloaded through the same workflow. 
```
biosample SAMN46714602 (SRA: SRS24009474)
biosample SAMN46547906 (SRA: SRS23982952)
biosample SAMN46547933 (SRA: SRS23982976)
```

### 3. Quality Control and Filtering
Most of the data we received were already trimmed, but a small subset was not. These reads were trimmed with trimmomatic v0.39. 


Numthreads=40
reference_dir=/global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference
bwa aln -t ${Numthreads} ${reference_dir}/Kronos -f ${accession}.${x}.sai ${accession}.${x}.filtered.fq
bwa sampe -N 10 -n 10 -f ${accession}.sam ${reference_dir}/Kronos ${accession}.1.sai ${accession}.2.sai ${accession}.1.filtered.fq ${accession}.2.filtered.fq

### 3. Quality Control and Filtering

Reads are filtered using fastp v0.23.4 to remove low-quality reads:
```
for fq in *R1_*; do
    p1=$fq
    p2=$(echo $p1 | sed 's/R1/R2/g')
    prefix=$(echo $p1 | cut -d "_" -f 3)
    fastp --in1 ${p1} --in2 ${p2} --out1 ${prefix}.1.filtered.fq --out2 ${prefix}.2.filtered.fq --thread 16 -q 20
done
```
