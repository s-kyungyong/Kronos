## Exome capture sequencing data remapping 

Exome capture sequencing data was generated by [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and available through [PRJNA258539](https://www.ncbi.nlm.nih.gov/sra?linkname=bioproject_sra_all&from_uid=258539). There are 1,479 SRA experiments deposited under this BioProject. 39 of them are additional libraries deposited existing Kronos mutants. We are therefore targeting to analyze 1,440 mutants. We retrived the SRA accession of the experiments and obtained corresponding SRR accessions as below.

Obtain SRR accessions for the given SRX accessions.
```
#this can be done with a python script
from pysradb import SRAweb

for line in open('SRA.list', 'r'):
  mutant, srx = line.split('\t')
  srr = db.srx_to_srr([srx])['run_accession'].iloc[0]
  print(f'{mutant} {srx} {srr}')
```

This information is available in **SRA.list** and looks like this.
```
Mutant ID	SRA accession	SRR accession
Kronos0-1	SRX688079	SRR1559585
Kronos0-1	SRX688078	SRR1559584
Kronos0-2	SRX688080	SRR1559586
Kronos1000	SRX688377	SRR1559883
Kronos1002	SRX688378	SRR1559884
```


The SRR accessions (${srr}) obtained from the previous step can be set up as ${accession}. Download the sequencing data from the Sequence Read Archive with sra-toolkit v3.1.1. 
```
sratoolkit.3.1.1-centos_linux64/bin/prefetch ${accession}
sratoolkit.3.1.1-centos_linux64/bin/fasterq-dump -O . -e ${Numthreads} ${accession}
```

Filter the sequencing data with fastp v0.23.4.
```
fastp --in1 ${accession}_1.fastq --in2 ${accession}_2.fastq --out1 ${accession}.1.filtered.fq --out2 ${accession}.2.filtered.fq --thread 16 -q 20
```

Align the reads to the broken Kronos genome with bwa aln v0.7.18-r1243-dirty.
```
# ${x} is either 1 or 2 for R1 and R2 in the paired-end library
bwa aln -t ${Numthreads} ${reference_dir}/Kronos -f ${accession}.${x}.sai ${accession}.${x}.filtered.fq
bwa sampe -N 10 -n 10 -f ${accession}.sam ${reference_dir}/Kronos ${accession}.1.sai ${accession}.2.sai ${accession}.1.filtered.fq ${accession}.2.filtered.fq
```

Sort the alignment file with samtools v1.20 and remove duplicates with picard v3.0.0
```
samtools view -@ ${Numthreads} -h -b ${accession}.sam | samtools sort -@ ${Numthreads} > ${accession}.sorted.bam
samtools index ${accession}.sorted.bam

#occasionally, the bam file may contains records that picard does not like to process.
#ALIDATION_STRINGENCY=LENIENT can be added to skip those alignments
picard MarkDuplicates REMOVE_DUPLICATES=true I=${accession}.sorted.bam O=${accession}.sorted.rmdup.bam M=${accession}.rmdup.txt
```

Some mutants have two sequencing datasets deposited. Let's merge those into single bam files. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.sorted.rmdup.bam
 done < merge.list
```

By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. We try to process about 24 samples in a single batch. During Kronos mutant sequencing library peparation, a batch of 8 mutants was typically processed together and three batches were sequenced together. But there are some variations in number of mutants prepared, and some sequencing datasets are not available in the NCBI. As we do not want to split datasets from a single batch into different groups for the MAPS. The number of members in the processing groups will slightly vary. This information can be found in **MAPS_groups.list**.

```
#create folders and move bam files
for i in {1..60}; do mkdir MAPS-${i}; done
cat MAPS_groups.list | while read line; do
    accession=$(echo $line | awk '{print $4}')
    i=$(echo $line | awk '{print $6}')
    mv ${accession}.sorted.rmdup.bam MAPS-${i}/
done
```

For each folder, we will run [the MAPS pipeline](https://github.com/DubcovskyLab/wheat_tilling_pub).
```
#generate the mpileup outputs
python ./wheat_tilling_pub/maps_pipeline/beta-run-mpileup.py -t 30 -r Kronos.collapsed.chromosomes.masked.v1.1.broken.fa -q 20 -Q 20 -o Kronos_mpileup.txt -s $(which samtools) --bamname .sorted.rmdup.bam

#process the mpileup outputs
mkdir MAPS && cd MAPS
for idx in {2..30}; do # 28 chromosomes + unplaced 
    bed="../temp-region-${idx}.bed"
    mpileup="../temp-mpileup-part-${idx}.txt"

    # Read the first line only and get the first field (chromosome)
    chr=$(awk 'NR==1 {print $1}' "$bed")

    # Check if the chromosome directory exists, if not, create it
    [ ! -d "${chr}" ] && mkdir -p "${chr}"

    # Copy the mpileup file into the appropriate chromosome directory
    # Add error checking for the copy operation
    head -n 1 ../Kronos_mpileup.txt > "${chr}/${chr}_mpileup.txt"
    cat "$mpileup" >> "${chr}/${chr}_mpileup.txt"
done

#run the first part of the MAPS pipeline
#the paramter l was (# libraries - 4)
for chr in $(ls -d */ | sed 's/\/$//'); do
    pushd "$chr"  
    python ./wheat_tilling_pub/maps_pipeline/beta-mpileup-parser.py -t 56 -f "${chr}_mpileup.txt"
    python ./wheat_tilling_pub/maps_pipeline/beta-maps1-v2.py -f "parsed_${chr}_mpileup.txt" -t 56 -l ${l} -o "${chr}.mapspart1.txt"
    popd
done

#run the second part of the MAPS pipeline
cd .. && mkdir MAPS_output && cd MAPS_output
head -n 1 ../MAPS/1A/1A.mapspart1.txt > all.mapspart1.out
#merge all outcomes
for file in ../MAPS/*/*.mapspart1.txt; do tail -n +2 "$file"; done >> all.mapspart1.out

#diversify two parameters
for pair in "2,3" "3,2", "3,4" "3,5" "4,3" "5,3" "6,4"; do
  k=$(echo $pair | cut -d',' -f1)
  j=$(echo $pair | cut -d',' -f2)
  python ./wheat_tilling_pub/maps_pipeline/maps-part2-v2.py -f all.mapspat1.txt --hetMinPer 15 -l $l --homMinCov $k --hetMinCov $j -o all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.tsv -m m
done
```

Once all folders were processed, all outputs were merged together.
```
for tsv in MAPS-1/MAPS_output/*.tsv; do
  cat MAPS-*/MAPS_output/${tsv} > ${tsv}
done
```

Then, convert the SRR accessions to proper Kronos mutant names and the tsv files to vcf formats. 
```
ls *.tsv | while read line; do python reformat_maps2_tsv.py $line ; done
ls *.reformatted.tsv | while read line; bash ./wheat_tilling_pub/postprocessing/residual_heterogeneity/generate_RH.sh $line chr.length.list; done

mkdir no_RH
mv *No_RH.maps* no_RH/ && cd no_RH/
bash wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh

mkdir RH
mv *RH_only* RH/ && cd RH
bash ./wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh
```

Find the paramters that maximize the number of EMS-type substitutions while maintaining EMS mutation rates > 98%. Then, generate the final outputs by collecting mutations detected using those parameters.
```
python summarize_vcf.py
python ../final_vcf.py combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.vcf No_RH.maps.vcf
python ../final_vcf.py combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.vcf RH_only.maps.vcf
```
java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2 combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.vcf > 
combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.No_RH.maps.snpeff.vcf
java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2 combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.vcf > combined.mapspart2.Lib20HetMinPer15HetMinCovVariableHomMinCovVariable.reformatted.corrected.10kb_bins.RH.byContig.MI.RH_only.maps.snpeff.vcf
Finally, let's run varient effect predictions using the v2 annotation set. 



GATK


start=$1
end=$2

# List all bam files and process them with a zero-based index
i=0
for bam in *.gatk.sorted.bam; do
    # Check if the current index falls within the specified range
    if (( i >= start && i < end )); then
        prefix=$(echo "$bam" | cut -d "." -f 1)

        # Check if the output file already exists
        if [ ! -f "${prefix}.gatk.sorted.rmdup.bam" ]; then
            picard MarkDuplicates REMOVE_DUPLICATES=true \
                I="${prefix}.gatk.sorted.bam" \
                O="${prefix}.gatk.sorted.rmdup.bam" \
                M="${prefix}.rmdup.txt"
            samtools index -@ 56 ${prefix}.gatk.sorted.rmdup.bam
        fi
    fi
    ((i++))  # Increment the index
done

Some mutants have two sequencing datasets deposited. Let's merge those into single bam files. 
```
while read lib1 lib2; do
     samtools merge -@ 56 -o ../${lib1}.gatk.sorted.rmdup.bam ${lib1}.gatk.sorted.rmdup.bam ${lib2}.gatk.sorted.rmdup.bam
 done < merge.list
```

We can then run HaplotypeCaller from GATK v4.5.0

start=$1
end=$2

# List all bam files and process them with a zero-based index
i=0
for bam in *.gatk.sorted.rmdup.bam; do
    # Check if the current index falls within the specified range
    if (( i >= start && i < end )); then
        prefix=$(echo "$bam" | cut -d "." -f 1)
        output=${prefix}.vcf.gz
        # Check if the output file already exists
        if [ ! -f "${output}" ]; then
            samtools index -@ 56 ${bam}
            gatk HaplotypeCaller -R /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos.collapsed.chromosomes.masked.v1.1.broken.fa -I ${bam} -O ${output}
        fi
    fi
    ((i++))  # Increment the index
done

