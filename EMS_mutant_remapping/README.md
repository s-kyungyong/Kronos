# Capture sequencing data remapping 


## Data availability 
Exome and promoter capture data come from [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and [Zhang et al., 2023](https://www.pnas.org/doi/10.1073/pnas.2306494120).
```
Exome capture: PRJNA258539
Promoter capture: PRJNA1218005 
```

Variants detected by remapping to the Kronos reference genome are available in Zenodo. The annotation v2.1 was used to retrieve mutation effects. 
```
```


## Methods

## Software version

```
sra-toolkit v3.1.1
fastp v0.23.4
trimmomatic v0.39
bwa v0.7.18-r1243-dirty
samtools v1.2
picard v3.0.0
MAPS
GATK v4.5.0
snpeff v5.2a
```
---

## Exome-capture

Exome capture sequencing data was generated by [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114) and is available through [PRJNA258539](https://www.ncbi.nlm.nih.gov/sra?linkname=bioproject_sra_all&from_uid=258539). This BioProject contains 1,479 SRA experiments in which 39 libraries are for redundant Kronos mutants. These redundant libraries will be merged, resulting in 1,440 unique mutants for analysis. The datasets are processed with (A) the MAPS pipeline and (B) GATK, separately. 

### 1. Downloading Sequencing Data

SRA and SRR identifiers for each mutant is available in **exome_SRA.list**. The SRR accessions is now referred to as ${accession}. Sequencing data is downloaded from the SRA using sra-toolkit. 
```
sratoolkit.3.1.1-centos_linux64/bin/prefetch ${accession}
sratoolkit.3.1.1-centos_linux64/bin/fasterq-dump -O . -e ${Numthreads} ${accession}
```

### 2. Quality Control and Filtering

Reads are filtered using fastp v0.23.4 to remove low-quality reads:
```
fastp --in1 ${accession}_1.fastq --in2 ${accession}_2.fastq --out1 ${accession}.1.filtered.fq --out2 ${accession}.2.filtered.fq --thread 16 -q 20
```

## A. The MAPS pipeline

### 3A. Read Alignment
Reads are aligned to the broken Kronos genome using bwa v0.7.18-r1243-dirty:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

# generate sai and convert it to sam
for i in 1 2; do
    bwa aln -t ${Numthreads} ${reference_dir}/Kronos -f ${accession}.${x}.sai ${accession}.${x}.filtered.fq
done
bwa sampe -N 10 -n 10 -f ${accession}.sam ${reference_dir}/Kronos ${accession}.1.sai ${accession}.2.sai ${accession}.1.filtered.fq ${accession}.2.filtered.fq
```

### 4A. Sorting and Deduplication
Alignment files are sorted using samtools and duplicates are removed with picard:
```
samtools view -@ ${Numthreads} -h -b ${accession}.sam | samtools sort -@ ${Numthreads} > ${accession}.sorted.bam
samtools index ${accession}.sorted.bam

#occasionally, the bam file may contains records that picard does not like to process.
#ALIDATION_STRINGENCY=LENIENT can be added to skip those alignments
picard MarkDuplicates REMOVE_DUPLICATES=true I=${accession}.sorted.bam O=${accession}.sorted.rmdup.bam M=${accession}.rmdup.txt
```

### 5A. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. These are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. Note that *.sorted.rmdup.bam files are required for Step 10A. Do not delete them yet! 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.sorted.rmdup.bam
 done < exome_merge.list
```

### 6A. Preparing for MAPS Pipeline
Mutants were prepared for squencing in batches of about 8, and 3 batches were sequenced together, though some variations exist. To keep each batch together for MAPS processing, we create separate folders based on **exome_MAPS_groups.list**:
```
#create folders and move bam files
for i in {1..60}; do mkdir MAPS-${i}; done

# Move BAM files into their respective groups
cat exome_MAPS_groups.list | while read line; do
    accession=$(echo $line | awk '{print $4}')
    i=$(echo $line | awk '{print $6}')
    mv ${accession}.sorted.rmdup.bam MAPS-${i}/
done
```

### 7A. Running the MAPS Pipeline
For each folder, we will run [the MAPS pipeline](https://github.com/DubcovskyLab/wheat_tilling_pub). First, generate mpileup outputs.
```
python ./wheat_tilling_pub/maps_pipeline/beta-run-mpileup.py \
      -t 30 -r Kronos.collapsed.chromosomes.masked.v1.1.broken.fa \
      -q 20 -Q 20 -o Kronos_mpileup.txt -s $(which samtools) \
      --bamname .sorted.rmdup.bam
```

Then, process the mpileup outputs.
```
mkdir MAPS && cd MAPS
for idx in {2..30}; do # 28 chromosomes + unplaced 
    bed="../temp-region-${idx}.bed"
    mpileup="../temp-mpileup-part-${idx}.txt"

    # Read the first line only and get the first field (chromosome)
    chr=$(awk 'NR==1 {print $1}' "$bed")

    # Check if the chromosome directory exists, if not, create it
    [ ! -d "${chr}" ] && mkdir -p "${chr}"

    # Copy the mpileup file into the appropriate chromosome directory
    # Add error checking for the copy operation
    head -n 1 ../Kronos_mpileup.txt > "${chr}/${chr}_mpileup.txt"
    cat "$mpileup" >> "${chr}/${chr}_mpileup.txt"
done
```

Run the first part of the MAPS pipeline. Here, set l as **(# libraries - 4)**.
```
for chr in $(ls -d */ | sed 's/\/$//'); do
    pushd "$chr"  
    python ./wheat_tilling_pub/maps_pipeline/beta-mpileup-parser.py -t 56 -f "${chr}_mpileup.txt"
    python ./wheat_tilling_pub/maps_pipeline/beta-maps1-v2.py -f "parsed_${chr}_mpileup.txt" -t 56 -l ${l} -o "${chr}.mapspart1.txt"
    popd
done
```

Run the second part of the MAPS pipeline.
```
cd .. && mkdir MAPS_output && cd MAPS_output
head -n 1 ../MAPS/1A/1A.mapspart1.txt > all.mapspart1.out
#merge all outcomes
for file in ../MAPS/*/*.mapspart1.txt; do tail -n +2 "$file"; done >> all.mapspart1.out

#diversify two parameters
#each pair is homMinCov,hetMinCov
for pair in "2,3" "3,4" "3,5" "4,6"; do
  k=$(echo $pair | cut -d',' -f1)
  j=$(echo $pair | cut -d',' -f2)
  python ./wheat_tilling_pub/maps_pipeline/maps-part2-v2.py -f all.mapspat1.txt --hetMinPer 15 -l $l --homMinCov $k --hetMinCov $j -o all.mapspart2.Lib20HetMinPer15HetMinCov${j}HomMinCov${k}.tsv -m m
done
```

### 8A. Vcf Conversion and Residual Hetrogenity Filtering
Once all folders were processed, all outputs were merged together. Make sure that all 1,440 library data is present here. 
```
for tsv in MAPS-1/MAPS_output/*.tsv; do
  cat MAPS-*/MAPS_output/${tsv} > ${tsv}
done
```

Then, convert the SRR accessions to proper Kronos mutant names and the tsv files to vcf formats. 
```
ls *.tsv | while read line; do python reformat_maps2_tsv.py $line exome_MAPS_groups.list; done
ls *.reformatted.tsv | while read line; bash ./wheat_tilling_pub/postprocessing/residual_heterogeneity/generate_RH.sh $line chr.length.list; done
```

Separate regions with residual hetrogenity.
```
#the outputs from this step will be the main ones. 
mkdir no_RH
mv *No_RH.maps* No_RH/ && cd No_RH/
bash wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh

mkdir RH
mv *RH_only* RH/ && cd RH
bash ./wheat_tilling_pub/postprocessing/vcf_modifications/fixMAPSOutputAndMakeVCF.sh
```

### 9A. Parameter Search
The MAPS outputs were generated with four pairs of HomMinCov and HetMinCov: HetMinCov3HomMinCov2, HetMinCov4HomMinCov3, HetMinCov5HomMinCov3 and HetMinCov6HomMinCov4 from the least to most stringency. Each parameter will be analyzed to select high, medium and low confidence thresholds. There are two major criteria. 

If any of the thresholds yielded ≥ 98% EMS rates, the following criteria are applied.
```
High confidence: the least stringent threshold yielding ≥ 98% EMS rates
Medium confidence: the least stringent threshold yielding ≥ 97% EMS rates among the remainning three, N/A otherwise
Low confidence: the least stringent threshold yielding ≥ 95% EMS rates among the remainning ones, N/A otherwise
```

If none of the thresholds yielded ≥ 98% EMS rates, pre-defined confidence levels will be used as [Krasileva et al., 2017](https://www.pnas.org/doi/10.1073/pnas.1619268114).
```
High confidence: HetMinCov5HomMinCov3
Medium confidence: HetMinCov4HomMinCov3
Low confidence: HetMinCov3HomMinCov2
```

Summarize the statistics and select the parameters. This will create Mutations. summary
```
#run this within the No_RH folder.
python summarize_vcf.py
```

### 10A. Rescuing Multi-mapped Reads

Almost everything is ready. We finally need to rescue multi-mapped reads. This step goes back to Step 5A and requires *.sorted.rmdup.bam files. Multi-mapped reads are rescored, so that any meaningful mutations can be picked up by the MAPS pipeline. 
```
for bam in *.sorted.rmdup.bam; do
    prefix="${bam%.bam}"

    # Convert BAM to SAM and extract header
    samtools view -@56 -h "${bam}" > "${prefix}.sam"
    samtools view -H "${prefix}.sam" > header.sam

    # Run multi-map corrector
    python ./wheat_tilling_pub/postprocessing/multi_map/multi-map-corrector-V1.6.py -i "${prefix}.sam" -l broken_chr.length.list > "${prefix}.rescued.sam"

    # Merge header and body, then convert to BAM
    cat header.sam "${prefix}.rescued.sam" | samtools view -@ 56 -b -o "${prefix}.rescued.bam"

    # Clean up temporary files
    rm "${prefix}.sam" "${prefix}.rescued.sam" header.sam
done
```

Once these files are generated, the entire MAPS pipeline from 6A to 8A is rerun for them. 

### 11A. Finalization

All outputs are concatnated into four final outputs. With the folders structured as following:
```
ls TSVs #uniquely mapped reads
No_RH  RH_only

ls TSVs-Multi #multi-mapped reads
No_RH  RH_only
```


Run the script.
```
python finalize_vcf.py
```

We will create four final outputs! 
```
# Excluding genomic regtions with residual hetrogenity. Indels only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.No_RH.maps.indels.snpeff.vcf
# Excluding genomic regtions with residual hetrogenity. Substitutions only ------- This is our primary output 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.No_RH.maps.substitutions.snpeff.vcf
# Only genomic regtions with residual hetrogenity. Indels only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.RH_only.maps.indels.snpeff.vcf
# Only genomic regtions with residual hetrogenity. Substitutions only 
Kronos_v1.1.Exom-capture.corrected.deduped.10kb_bins.RH.byContig.MI.RH_only.maps.substitutions.snpeff.vcf
```

Within these vcf files, the following fields will be included. Our primary mutations are labeled with Substitution/False/High/Any threshold/Unique. 
```
Type={Substiution/Indel}: This indicates whether called mutations are substitutions or indels. 
RH={True/False}: This indicates whether called mutations come from regions associated with residual hetrogenity.  
Confidence={High/Medium/Low}: This indicates the confidence level of called mutations.
Threshold=HetMinCov{x}HomMinCov{y}: This indicates parameters used to call the mutations. 
Mapping={Unique/Multi}: This indicates whether called mutations come from uniquely mapped or multi-mapped reads. 
```

### 12A. snpEff

Finally, snpEff is run to add variant effect prediction outputs. This is based on the annotation v21. 
```
for vcf in *.vcf:
    prefix="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv21 ${vcf} > ${prefix}.snpeff.vcf
done
```


## B. The GATK pipeline

To call variants with GATK, we begin with the trimmed filtered sequencing outputs from Step 2. 

### 3B. Read Alignment
Reads are aligned to the broken Kronos genome using bwa mem:
```
#information about how the genome was broken can be found
#https://github.com/krasileva-group/Kronos_EDR
bwa index Kronos Kronos.collapsed.chromosomes.masked.v1.1.broken.fa

for read1 in *.1.filtered.fq; do
  accession="${read1%.1.filtered.fq}"
  read2="${accession}.2.filtered.fq"

  # Add read group information during alignment
  RG="@RG\tID:${accession}\tLB:${accession}\tPL:ILLUMINA\tPU:unit1\tSM:${accession}"
  bwa mem -t 56 -R "$RG" /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos "${read1}" "${read2}" > "${accession}.gatk.sam"
```

### 4B. Sorting and Deduplication
Alignment files are sorted using samtools and duplicates are removed with picard:
```
  samtools view -@56 -h "${accession}.gatk.sam" | samtools sort -@56 -o "${accession}.gatk.sorted.bam"
  samtools index -@56 "${accession}.gatk.sorted.bam"
  picard MarkDuplicates REMOVE_DUPLICATES=true I="${accession}.gatk.sorted.bam" O="${accession}.gatk.sorted.rmdup.bam" M="${accession}.rmdup.txt"
```

### 5B. Merging Redundant Libraries
Some mutants have multiple sequencing datasets. these are merged into a single BAM file. By this step, we have 1,440 bam files ready to be processed with the MAPS pipeline. 
```
while read mutant lib1 lib2; do
     samtools merge -@ 56 -o ../sorted.rmdup.bam_files/${lib1}.gatk.sorted.rmdup.bam ${lib1}.sorted.rmdup.bam ${lib2}.gatk.sorted.rmdup.bam
 done < exome_merge.list
```

### 6B. Running HaplotypeCaller
SNPs are called by HaplotypeCaller from GATK, and positions are adjusted for the intact chromosomes.
```
for bam in *.gatk.sorted.rmdup.bam; do
  accession="${bam%.gatk.sorted.rmdup.bam}"
  gatk HaplotypeCaller -R /global/scratch/projects/vector_kvklab/KS-Kronos_remapping/Reference/Kronos.collapsed.chromosomes.masked.v1.1.broken.fa -I ${accession}.gatk.sorted.rmdup.bam -O ${accession}.vcf.gz
  gunzip ${accession}.vcf.gz
  python reformat_gatk.py ${accession}.vcf
done
```

### 7B. Variant Effect Prediction
Variant effect prediction is run on the annotation version 2.1. 
```
for vcf in Kronos*.vcf; do
    accession="${vcf%.vcf}"
    java -jar /global/scratch/users/skyungyong/Software/snpEff/snpEff.jar eff -v Kronosv2  ${vcf}> ${accession}.gatk.exome.snpeff.vcf
done
```

---


# Promoter capture sequencing data remapping 

The analysis of promoter-capture sequencing data was done nearly identically with the exome capture data analysis. Thus, only the workflow different from the previous one is recorded below. 


### 1. Downloading Sequencing Data
Most of the sequencing data were directly obtained from [Zhang et al. 2023](https://www.pnas.org/doi/10.1073/pnas.2306494120). Any missing sequencing data were downloaded from PRJNA1218005.

### 2. Quality Control and Filtering
The datasets in PRJNA1218005 should already be trimmed. A small number of datasets directly provided by the author was not. These were trimmed with trimmomatic. 

```
for fq1 in *R1*fastq.gz; do
        fq2=$(echo $fq1 | sed 's/R1/R2/g')
        out1=$(echo $fq1 | sed 's/.fastq.gz/_trimmed.fq.gz/g')
        out2=$(echo $fq2 | sed 's/.fastq.gz/_trimmed.fq.gz/g')

        if [[ ! -f "${out1}" ]]; then
                trimmomatic PE $fq1 $fq2 $out1 ${fq1}_trimmed_unpaired.fq.gz $out2 ${fq2}_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 SLIDINGWINDOW:4:20
        fi
done
```

### 5A. Merging Redundant Libraries

Although some Kronos mutants were sequenced multiple times, we did not concatenate redundant libraries into single files, unlike the approach taken for the exome capture data. Each batch and the mutant libraries within it were clearly defined by Junli et al., and we wanted to preserve this original structure. As a result, datasets generated from the same mutant were only merged at the final processing stage. Overall, 1,518 mutants have a single associated sequencing dataset, and 38 mutants have two. 

For GATK outputs, finish running Step 6B. Then, the vcf files can be merged by looking for unions of mutations:
```
awk '{print $2}' promoter_MAPS_group.list | grep 'Kronos' | sort | uniq -c | awk '$1 != 1 {print $2}' > redundant.list
while read -r accession; do
    libs=$(ls */${accession}.reformatted.vcf)
    for lib in ${libs}; do
            bgzip -c $lib > ${lib}.gz
            tabix --csi -p vcf ${lib}.gz
    done
    
    bcftools isec -o merged/${accession}.merged.reformatted.vcf.gz -w 1 -Oz -n =2 ${libs}
    bcftools norm -m -both merged/${accession}.merged.reformatted.vcf.gz -f Kronos.collapsed.chromosomes.masked.v1.1.fa -Ov -o merged/${accession}.merged.norm.reformatted.vcf
done < redundant.list
```

### 6A. Preparing for MAPS Pipeline

The BioProject PRJNA1218005 contains 33 BioSamples. Each BioSample contains a batch of Kronos mutants to be processed together. 

### 7A. Running the MAPS Pipeline

In first part of the MAPS pipeline, l was set as **int(math.ceil(lib * 0.8))**.
